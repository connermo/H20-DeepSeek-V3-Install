# ç‰ˆæœ¬ä¿¡æ¯æ±‡æ€»

> æœ€åæ›´æ–°: 2025å¹´12æœˆ24æ—¥

## ğŸ“¦ å®˜æ–¹æ¨èç‰ˆæœ¬

| ç»„ä»¶ | ç‰ˆæœ¬ | è¯´æ˜ |
|------|------|------|
| **CUDA** | 13.0 | æ¨èç¨³å®šç‰ˆ |
| **NVIDIAé©±åŠ¨** | 580.65.06+ <br>580.88+ (Windows) | R580ç³»åˆ—ï¼ŒCUDA 13.0å¿…éœ€ |
| **vLLM** | 0.13.0+ | åŒ…å«FP8ä¼˜åŒ– |
| **PyTorch** | 2.x | æœ€æ–°ç¨³å®šç‰ˆ |
| **Python** | 3.9-3.12 | æ¨è3.10+ |
| **Ubuntu** | 22.04 LTS | æ¨èLTSç‰ˆæœ¬ |
| **NCCL** | 2.x | æœ€æ–°ç‰ˆ |

## âš¡ Deepseek V3ä¸“ç”¨é…ç½®

| é…ç½®é¡¹ | æ¨èå€¼ | è¯´æ˜ |
|--------|--------|------|
| **æ¨¡å‹æ ¼å¼** | FP8 | åŸç”Ÿè®­ç»ƒæ ¼å¼ |
| **dtype** | auto | è‡ªåŠ¨æ£€æµ‹FP8 |
| **kv_cache_dtype** | fp8 | èŠ‚çœ50%æ˜¾å­˜ |
| **attention_backend** | FLASHINFER | FP8ä¼˜åŒ– |
| **tensor_parallel_size** | 8 | 8å¡å¹¶è¡Œ |
| **gpu_memory_utilization** | 0.95 | æ˜¾å­˜ä½¿ç”¨ç‡ |
| **max_model_len** | 8192 | ä¸Šä¸‹æ–‡é•¿åº¦ |

## ğŸ”„ ç‰ˆæœ¬å…¼å®¹æ€§çŸ©é˜µ

### CUDAä¸é©±åŠ¨å…¼å®¹æ€§

| CUDAç‰ˆæœ¬ | æœ€ä½é©±åŠ¨ç‰ˆæœ¬  | æœ€ä½é©±åŠ¨ç‰ˆæœ¬ (Windows) | é©±åŠ¨ç³»åˆ— |
|----------|---------------------|----------------------|----------|
| 13.0 | 580.65.06 | 580.88 | R580 |
| 12.9 | 550.xx | 553.xx | R550 |
| 12.8 | 545.xx | 546.xx | R545 |

### vLLMä¸PyTorchå…¼å®¹æ€§

| vLLMç‰ˆæœ¬ | PyTorchç‰ˆæœ¬ | CUDAç‰ˆæœ¬ | ç‰¹æ€§ |
|----------|-------------|----------|------|
| 0.13.0 | 2.x | 13.0/12.x | FP8ä¼˜åŒ–ï¼ŒDeepseek V3æ”¯æŒ |
| 0.12.x | 2.x | 12.x | åŸºç¡€æ”¯æŒ |

## ğŸ“Š æ€§èƒ½æŒ‡æ ‡

### Deepseek V3 (FP8 vs BF16)

| æŒ‡æ ‡ | FP8 | BF16 | æ”¹è¿› |
|------|-----|------|------|
| **æ˜¾å­˜å ç”¨** | ~700GB | ~1400GB | 50%â†“ |
| **ååé‡** | åŸºå‡†Ã—3 | åŸºå‡†Ã—1 | 300%â†‘ |
| **å»¶è¿Ÿ** | ä½ | ä¸­ | 30%â†“ |
| **ç²¾åº¦** | é«˜ | æ›´é«˜ | ç•¥ä½ |

### H20 8å¡é¢„æœŸæ€§èƒ½

- **ååé‡**: 500-1000 tokens/ç§’ (FP8)
- **é¦–tokenå»¶è¿Ÿ**: 0.5-2ç§’
- **æœ€å¤§å¹¶å‘**: 100+ è¯·æ±‚
- **æ˜¾å­˜ä½¿ç”¨**: 700-900GB (FP8)
- **NVLinkå¸¦å®½**: 400+ GB/s (8å¡æ€»è®¡)

## ğŸ”— å®˜æ–¹æ–‡æ¡£é“¾æ¥

- [CUDA 13.0ä¸‹è½½](https://developer.nvidia.com/cuda-downloads)
- [NVIDIAé©±åŠ¨ä¸‹è½½](https://www.nvidia.com/Download/index.aspx)
- [vLLM GitHub](https://github.com/vllm-project/vllm)
- [Deepseek V3 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-V3)
- [vLLM Deepseek V3æŒ‡å—](https://docs.vllm.ai/projects/recipes/en/latest/DeepSeek/DeepSeek-V3.html)

## âœ… å¿«é€Ÿæ£€æŸ¥å‘½ä»¤

```bash
# æ£€æŸ¥CUDAç‰ˆæœ¬
nvcc --version | grep release

# æ£€æŸ¥NVIDIAé©±åŠ¨
nvidia-smi --query-gpu=driver_version --format=csv,noheader

# æ£€æŸ¥vLLMç‰ˆæœ¬
python -c "import vllm; print(vllm.__version__)"

# æ£€æŸ¥PyTorchç‰ˆæœ¬
python -c "import torch; print(torch.__version__)"

# æ£€æŸ¥Pythonç‰ˆæœ¬
python --version

# æ£€æŸ¥GPUæ•°é‡
nvidia-smi --query-gpu=name --format=csv,noheader | wc -l

# æ£€æŸ¥NVLinkçŠ¶æ€
nvidia-smi nvlink --status
```

## ğŸ“ ç‰ˆæœ¬æ›´æ–°è®°å½•

- **2025-12-24**: 
  - æ›´æ–°CUDAè‡³13.0
  - æ›´æ–°é©±åŠ¨è‡³580.65.06/580.88
  - æ›´æ–°vLLMè‡³0.13.0
  - æ·»åŠ Deepseek V3 FP8åŸç”Ÿæ”¯æŒ
  - æ‰€æœ‰æ–‡æ¡£ç‰ˆæœ¬å·å¯¹é½

---

**æ³¨æ„**: æ­¤æ–‡æ¡£éšè½¯ä»¶æ›´æ–°æŒç»­ç»´æŠ¤ï¼Œè¯·å®šæœŸæŸ¥çœ‹æœ€æ–°ç‰ˆæœ¬ã€‚
