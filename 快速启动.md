# âš¡ H20 + vLLM + Deepseek V3 å¿«é€Ÿå¯åŠ¨å¡ç‰‡

> æ‰“å°æˆ–æ”¶è—æ­¤é¡µé¢ï¼Œéšæ—¶æŸ¥çœ‹å…³é”®å‘½ä»¤

---

## ğŸ¯ ä¸‰ç§åœºæ™¯ï¼Œä¸‰ç§å¯åŠ¨æ–¹å¼

### åœºæ™¯A: å…¨æ–°æœåŠ¡å™¨ (ä»€ä¹ˆéƒ½æ²¡æœ‰)

```bash
# 1. å®‰è£…Ubuntu 22.04 LTS (å‚è€ƒ: å®Œæ•´ç³»ç»Ÿé…ç½®æŒ‡å—.md)
#    - åˆ¶ä½œUç›˜å¯åŠ¨ç›˜
#    - æŒ‰å‘å¯¼å®‰è£…ç³»ç»Ÿ
#    - é…ç½®ç½‘ç»œå’ŒSSH

# 2. å®‰è£…NVIDIAé©±åŠ¨ (550ç³»åˆ—ï¼Œæ”¯æŒH20)
sudo add-apt-repository ppa:graphics-drivers/ppa -y
sudo apt update
sudo ubuntu-drivers autoinstall
sudo reboot

# 3. å®‰è£…CUDA 13.0 (æ¨èç¨³å®šç‰ˆ)
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb
sudo apt update
sudo apt install -y cuda-toolkit-13-0

# 4. é…ç½®ç¯å¢ƒå˜é‡
echo 'export CUDA_HOME=/usr/local/cuda-13.0' >> ~/.bashrc
echo 'export PATH=$CUDA_HOME/bin:$PATH' >> ~/.bashrc
echo 'export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc
source ~/.bashrc

# 5. éªŒè¯GPU
nvidia-smi
nvcc --version

# 6. ç»§ç»­åœºæ™¯B
```

**é¢„è®¡æ—¶é—´:** 2-4å°æ—¶

---

### åœºæ™¯B: å·²æœ‰Ubuntuå’Œé©±åŠ¨ (éœ€è¦éƒ¨ç½²vLLM)

```bash
# 1. å…‹éš†æˆ–ä¸‹è½½éƒ¨ç½²è„šæœ¬åˆ°æœåŠ¡å™¨
cd ~
mkdir -p H20_vllm && cd H20_vllm

# 2. é…ç½®vLLMç¯å¢ƒ
./setup_environment.sh

# 3. ä¸‹è½½æ¨¡å‹ (~300GBï¼Œéœ€è¦æ—¶é—´)
./download_model.sh
# æç¤º: é€‰æ‹©é•œåƒç«™ç‚¹å¯åŠ é€Ÿä¸‹è½½

# 4. å¯åŠ¨æ¨ç†æœåŠ¡
./start_inference.sh
# é»˜è®¤ç«¯å£: 8000
# é»˜è®¤é…ç½®: 8å¡å¹¶è¡Œ, bfloat16

# 5. æ–°ç»ˆç«¯æµ‹è¯•
python test_inference.py --mode api
```

**é¢„è®¡æ—¶é—´:** 1-3å°æ—¶ (ä¸»è¦æ˜¯æ¨¡å‹ä¸‹è½½)

---

### åœºæ™¯C: å·²éƒ¨ç½²vLLM (æ—¥å¸¸å¯åŠ¨)

```bash
# 1. æ¿€æ´»ç¯å¢ƒ
cd ~/H20_vllm
source vllm-env/bin/activate
source vllm_env.sh

# 2. å¯åŠ¨æœåŠ¡
./start_inference.sh

# æˆ–ä½¿ç”¨systemd (å¦‚æœå·²é…ç½®)
sudo systemctl start vllm-deepseek
```

**é¢„è®¡æ—¶é—´:** 1-2åˆ†é’Ÿ

---

## ğŸ”¥ æœ€å¸¸ç”¨å‘½ä»¤ (æ”¶è—)

### å¯åŠ¨å’Œåœæ­¢

```bash
# å¯åŠ¨vLLMæœåŠ¡
./start_inference.sh

# åœæ­¢æœåŠ¡ (åœ¨è¿è¡Œç»ˆç«¯æŒ‰)
Ctrl + C

# æˆ–æŸ¥æ‰¾è¿›ç¨‹å¹¶æ€æ­»
lsof -i :8000
kill -9 <PID>

# systemdæ–¹å¼ (å¦‚æœé…ç½®äº†)
sudo systemctl start vllm-deepseek    # å¯åŠ¨
sudo systemctl stop vllm-deepseek     # åœæ­¢
sudo systemctl status vllm-deepseek   # æŸ¥çœ‹çŠ¶æ€
sudo systemctl restart vllm-deepseek  # é‡å¯
```

### ç›‘æ§

```bash
# GPUå®æ—¶ç›‘æ§
nvidia-smi                           # å•æ¬¡æŸ¥çœ‹
watch -n 1 nvidia-smi               # æ¯ç§’åˆ·æ–°
./monitor_gpu.sh                    # äº¤äº’å¼ç›‘æ§

# æŸ¥çœ‹æ—¥å¿—
tail -f vllm_server.log             # å®æ—¶æ—¥å¿—
less vllm_server.log                # æŸ¥çœ‹å®Œæ•´æ—¥å¿—

# ç³»ç»ŸéªŒè¯
~/system_validation.sh              # å®Œæ•´æ€§æ£€æŸ¥
```

### æµ‹è¯•

```bash
# åŠŸèƒ½æµ‹è¯•
python test_inference.py --mode direct    # ç›´æ¥æ¨ç†
python test_inference.py --mode api       # APIæ¨¡å¼

# æ€§èƒ½æµ‹è¯•
python benchmark.py                       # å®Œæ•´æµ‹è¯•
python benchmark.py --num-prompts 10      # å¿«é€Ÿæµ‹è¯•

# è‡ªå®šä¹‰æµ‹è¯•
python test_inference.py --mode api --custom-prompt "ä½ çš„é—®é¢˜"
```

### APIè°ƒç”¨

```bash
# ä½¿ç”¨curlæµ‹è¯•
curl http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{"model": "/data/models/deepseek-v3", "prompt": "ä½ å¥½", "max_tokens": 100}'

# æ£€æŸ¥æœåŠ¡å¥åº·
curl http://localhost:8000/health

# æŸ¥çœ‹æ¨¡å‹åˆ—è¡¨
curl http://localhost:8000/v1/models
```

---

## ğŸš¨ ç´§æ€¥æ•…éšœå¤„ç†

### GPUä¸å¯ç”¨

```bash
# 1. æ£€æŸ¥GPU
nvidia-smi

# 2. å¦‚æœå¤±è´¥ï¼Œé‡å¯é©±åŠ¨
sudo systemctl restart nvidia-persistenced

# 3. è¿˜æ˜¯ä¸è¡Œï¼Œé‡å¯æœåŠ¡å™¨
sudo reboot
```

### ç«¯å£è¢«å ç”¨

```bash
# æŸ¥çœ‹å ç”¨
lsof -i :8000

# æ€æ­»è¿›ç¨‹
kill -9 <PID>

# æˆ–ä½¿ç”¨å…¶ä»–ç«¯å£
./start_inference.sh  # ç„¶åè¾“å…¥æ–°ç«¯å£å¦‚8001
```

### æ˜¾å­˜ä¸è¶³ (OOM)

```bash
# ç¼–è¾‘å¯åŠ¨è„šæœ¬ï¼Œé™ä½å‚æ•°
# åœ¨start_inference.shä¸­ä¿®æ”¹:
--gpu-memory-utilization 0.85    # ä»0.95é™åˆ°0.85
--max-model-len 4096             # ä»8192é™åˆ°4096
```

### æœåŠ¡æ— å“åº”

```bash
# 1. æ£€æŸ¥è¿›ç¨‹
ps aux | grep vllm

# 2. æ£€æŸ¥GPUçŠ¶æ€
nvidia-smi

# 3. æŸ¥çœ‹æ—¥å¿—é”™è¯¯
tail -100 vllm_server.log | grep -i error

# 4. é‡å¯æœåŠ¡
# Ctrl+C ç„¶åé‡æ–°è¿è¡Œ ./start_inference.sh
```

---

## ğŸ“‹ ç³»ç»ŸçŠ¶æ€å¿«é€Ÿæ£€æŸ¥

```bash
# ä¸€é”®æ£€æŸ¥è„šæœ¬
cat << 'EOF' > ~/quick_check.sh
#!/bin/bash
echo "=== GPUçŠ¶æ€ ==="
nvidia-smi --query-gpu=index,name,utilization.gpu,memory.used,memory.total --format=csv

echo -e "\n=== vLLMæœåŠ¡ ==="
if lsof -Pi :8000 -sTCP:LISTEN -t >/dev/null ; then
    echo "âœ“ vLLMæœåŠ¡è¿è¡Œä¸­ (ç«¯å£8000)"
else
    echo "âœ— vLLMæœåŠ¡æœªè¿è¡Œ"
fi

echo -e "\n=== ç£ç›˜ç©ºé—´ ==="
df -h | grep -E "Filesystem|/$|/data"

echo -e "\n=== ç³»ç»Ÿè´Ÿè½½ ==="
uptime

echo -e "\n=== GPUæ¸©åº¦ ==="
nvidia-smi --query-gpu=index,temperature.gpu --format=csv,noheader
EOF

chmod +x ~/quick_check.sh
~/quick_check.sh
```

---

## ğŸ¨ Pythonå®¢æˆ·ç«¯ä»£ç æ¨¡æ¿

### åŸºç¡€è°ƒç”¨

```python
from openai import OpenAI

# åˆå§‹åŒ–å®¢æˆ·ç«¯
client = OpenAI(
    api_key="EMPTY",
    base_url="http://localhost:8000/v1"
)

# å¯¹è¯
response = client.chat.completions.create(
    model="/data/models/deepseek-v3",
    messages=[
        {"role": "user", "content": "ä½ å¥½ï¼Œä»‹ç»ä¸€ä¸‹è‡ªå·±"}
    ],
    max_tokens=200
)

print(response.choices[0].message.content)
```

### æµå¼è¾“å‡º

```python
from openai import OpenAI

client = OpenAI(api_key="EMPTY", base_url="http://localhost:8000/v1")

stream = client.chat.completions.create(
    model="/data/models/deepseek-v3",
    messages=[{"role": "user", "content": "å†™ä¸€ä¸ªPythonæ’åºç®—æ³•"}],
    stream=True,
    max_tokens=500
)

for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end='', flush=True)
```

### æ‰¹é‡å¤„ç†

```python
from vllm import LLM, SamplingParams

llm = LLM(model="/data/models/deepseek-v3", tensor_parallel_size=8)

prompts = ["é—®é¢˜1: ...", "é—®é¢˜2: ...", "é—®é¢˜3: ..."]
sampling_params = SamplingParams(temperature=0.7, max_tokens=256)

outputs = llm.generate(prompts, sampling_params)

for i, output in enumerate(outputs):
    print(f"ç»“æœ{i+1}: {output.outputs[0].text}")
```

---

## ğŸ“Š å…³é”®å‚æ•°é€ŸæŸ¥

### å¯åŠ¨å‚æ•°ï¼ˆDeepseek V3ä¸“ç”¨ï¼‰

```bash
--model                      # æ¨¡å‹è·¯å¾„
--tensor-parallel-size 8     # GPUå¹¶è¡Œæ•° (8å¡)
--dtype auto                # è‡ªåŠ¨æ£€æµ‹ï¼ˆFP8åŸç”Ÿæ ¼å¼ï¼‰
--kv-cache-dtype fp8        # KVç¼“å­˜ç±»å‹ï¼ˆFP8ï¼Œæ¨èï¼‰
--max-model-len 8192        # æœ€å¤§åºåˆ—é•¿åº¦
--gpu-memory-utilization 0.95  # æ˜¾å­˜ä½¿ç”¨ç‡
--port 8000                 # æœåŠ¡ç«¯å£
--host 0.0.0.0             # ç›‘å¬åœ°å€
```

**âš ï¸ Deepseek V3é‡è¦æç¤º**ï¼š
- åŸç”ŸFP8è®­ç»ƒï¼Œå¿…é¡»ä½¿ç”¨FP8æ ¼å¼
- FP8æ¯”BF16èŠ‚çœ50%æ˜¾å­˜ï¼Œæä¾›3xæ€§èƒ½
- KV cacheæ¨èä½¿ç”¨fp8ï¼ˆå¯é€‰bfloat16ï¼‰

### æ¨ç†å‚æ•°

```python
temperature: 0.7         # éšæœºæ€§ (0-2, è¶Šé«˜è¶Šéšæœº)
top_p: 0.9              # æ ¸é‡‡æ · (0-1)
max_tokens: 2048        # æœ€å¤§ç”Ÿæˆé•¿åº¦
repetition_penalty: 1.0 # é‡å¤æƒ©ç½š (1.0-2.0)
```

---

## ğŸ”— é‡è¦æ–‡ä»¶ä½ç½®

```
å·¥ä½œç›®å½•: ~/H20_vllm/
â”œâ”€â”€ vllm-env/              # Pythonè™šæ‹Ÿç¯å¢ƒ
â”œâ”€â”€ vllm_env.sh           # ç¯å¢ƒå˜é‡é…ç½®
â”œâ”€â”€ vllm_server.log       # æœåŠ¡æ—¥å¿—
â””â”€â”€ *.sh                  # å„ç§è„šæœ¬

æ¨¡å‹ç›®å½•: /data/models/deepseek-v3/
â”œâ”€â”€ config.json
â”œâ”€â”€ tokenizer.json
â”œâ”€â”€ *.safetensors         # æ¨¡å‹æƒé‡
â””â”€â”€ ...

é…ç½®æ–‡ä»¶:
â”œâ”€â”€ ~/.bashrc             # ç¯å¢ƒå˜é‡
â””â”€â”€ /etc/systemd/system/vllm-deepseek.service  # systemdæœåŠ¡(å¯é€‰)
```

---

## ğŸ“ è·å–å¸®åŠ©

```bash
# æŸ¥çœ‹å®Œæ•´æ–‡æ¡£
cat README.md
cat å®Œæ•´ç³»ç»Ÿé…ç½®æŒ‡å—.md

# æŸ¥çœ‹è¯¦ç»†å‘½ä»¤
cat QUICKREF.md

# æŸ¥çœ‹æ–‡æ¡£ç´¢å¼•
cat INDEX.md
```

---

## âœ… æ¯æ—¥æ£€æŸ¥æ¸…å•

**æ—©ä¸Šå¯åŠ¨:**
- [ ] `nvidia-smi` æ£€æŸ¥GPU
- [ ] `./start_inference.sh` å¯åŠ¨æœåŠ¡
- [ ] `curl http://localhost:8000/health` æµ‹è¯•API

**è¿è¡Œä¸­:**
- [ ] `watch -n 10 nvidia-smi` ç›‘æ§GPU
- [ ] `tail -f vllm_server.log` æŸ¥çœ‹æ—¥å¿—
- [ ] å®šæœŸæ£€æŸ¥ç£ç›˜ç©ºé—´ `df -h`

**æ™šä¸Šå…³é—­ (å¦‚éœ€è¦):**
- [ ] `Ctrl+C` åœæ­¢æœåŠ¡
- [ ] æˆ–ä¿æŒè¿è¡Œ (æ¨è)

---

## ğŸ’¡ æ€§èƒ½ä¼˜åŒ–æç¤º

1. **ååé‡ä¼˜åŒ–**: å¢åŠ batch size
   ```bash
   --max-num-seqs 256
   --max-num-batched-tokens 16384
   ```

2. **å»¶è¿Ÿä¼˜åŒ–**: å‡å°‘batch size
   ```bash
   --max-num-seqs 32
   ```

3. **æ˜¾å­˜ä¼˜åŒ–**: é™ä½ä½¿ç”¨ç‡
   ```bash
   --gpu-memory-utilization 0.85
   --max-model-len 4096
   ```

4. **ç¨³å®šæ€§ä¼˜åŒ–**: å¯ç”¨æŒä¹…åŒ–
   ```bash
   sudo nvidia-smi -pm 1
   ```

---

**è®°ä½: é‡åˆ°é—®é¢˜å…ˆæŸ¥æ—¥å¿— `tail -f vllm_server.log`** ğŸ”

**ä¿å­˜æ­¤æ–‡ä»¶åˆ°æ‰‹æœºæˆ–æ‰“å°å‡ºæ¥ï¼Œéšæ—¶æŸ¥é˜…ï¼** ğŸ“±ğŸ–¨ï¸
