# H20 8å¡æœºå™¨ vLLM + Deepseek V3 éƒ¨ç½²å·¥å…·åŒ…

æœ¬å·¥å…·åŒ…æä¾›åœ¨H20 8å¡æœºå™¨ä¸Šå¿«é€Ÿéƒ¨ç½²vLLMæ¨ç†å¼•æ“å’ŒDeepseek V3æ¨¡å‹çš„å®Œæ•´è§£å†³æ–¹æ¡ˆã€‚

## å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒé…ç½®ï¼ˆä¸€é”®å®Œæˆï¼‰

```bash
./scripts/setup_environment.sh
```

è¯¥è„šæœ¬ä¼šè‡ªåŠ¨å®Œæˆï¼š
- æ£€æŸ¥GPUå’ŒCUDAç¯å¢ƒ
- åˆ›å»ºPythonè™šæ‹Ÿç¯å¢ƒ
- å®‰è£…PyTorchå’ŒvLLM
- é…ç½®ç¯å¢ƒå˜é‡

### 2. ä¸‹è½½æ¨¡å‹

```bash
./scripts/download_model.sh
```

è¯¥è„šæœ¬ä¼šï¼š
- äº¤äº’å¼é€‰æ‹©æ¨¡å‹å­˜å‚¨è·¯å¾„
- å¯é€‰æ‹©ä½¿ç”¨å›½å†…é•œåƒç«™ç‚¹
- è‡ªåŠ¨ä¸‹è½½Deepseek V3æ¨¡å‹ï¼ˆ~300GBï¼‰
- éªŒè¯æ¨¡å‹å®Œæ•´æ€§

### 3. å¯åŠ¨æ¨ç†æœåŠ¡

```bash
./scripts/start_inference.sh
```

è¯¥è„šæœ¬ä¼šï¼š
- äº¤äº’å¼é…ç½®æœåŠ¡å‚æ•°ï¼ˆç«¯å£ã€å¹¶è¡Œåº¦ç­‰ï¼‰
- å¯åŠ¨OpenAIå…¼å®¹APIæœåŠ¡å™¨
- è‡ªåŠ¨è®°å½•æ—¥å¿—åˆ° `vllm_server.log`

### 4. æµ‹è¯•æ¨ç†

```bash
# æ–¹å¼1: ç›´æ¥æ¨ç†æ¨¡å¼
python scripts/test_inference.py --mode direct

# æ–¹å¼2: APIæ¨¡å¼ï¼ˆéœ€å…ˆå¯åŠ¨æœåŠ¡ï¼‰
python scripts/test_inference.py --mode api

# è‡ªå®šä¹‰prompt
python scripts/test_inference.py --mode direct --custom-prompt "ä½ çš„é—®é¢˜"
```

## æ–‡ä»¶è¯´æ˜

### æ–‡æ¡£
- **H20_VLLM_DeepSeek_V3_å®‰è£…æŒ‡å—.md** - è¯¦ç»†çš„å®‰è£…å’Œä½¿ç”¨æ–‡æ¡£

### è„šæœ¬å·¥å…· (scripts/ ç›®å½•)
- **setup_environment.sh** - ç¯å¢ƒä¸€é”®é…ç½®è„šæœ¬
- **download_model.sh** - æ¨¡å‹ä¸‹è½½è„šæœ¬
- **start_inference.sh** - æ¨ç†æœåŠ¡å¯åŠ¨è„šæœ¬
- **monitor_gpu.sh** - GPUå®æ—¶ç›‘æ§å·¥å…·
- **test_inference.py** - æ¨ç†åŠŸèƒ½æµ‹è¯•
- **benchmark.py** - æ€§èƒ½åŸºå‡†æµ‹è¯•

## å¸¸ç”¨å‘½ä»¤

### ç›‘æ§GPU

```bash
# å®æ—¶ç›‘æ§
./scripts/monitor_gpu.sh

# æˆ–ä½¿ç”¨nvidia-smi
watch -n 1 nvidia-smi
```

### æ€§èƒ½æµ‹è¯•

```bash
# è¿è¡ŒåŸºå‡†æµ‹è¯•
python scripts/benchmark.py --model-path /data/models/deepseek-v3

# è‡ªå®šä¹‰æµ‹è¯•å‚æ•°
python scripts/benchmark.py \
    --num-prompts 100 \
    --prompt-length 128 \
    --output-length 256 \
    --batch-sizes "1,4,8,16,32"
```

### APIè°ƒç”¨ç¤ºä¾‹

```bash
# ä½¿ç”¨curlæµ‹è¯•
curl http://localhost:8000/v1/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "/data/models/deepseek-v3",
        "prompt": "ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½",
        "max_tokens": 500
    }'
```

## é…ç½®è¯´æ˜

### å…³é”®å‚æ•°

- **tensor-parallel-size**: 8 ï¼ˆä½¿ç”¨8å¼ GPUï¼‰
- **dtype**: bfloat16 ï¼ˆå¹³è¡¡æ€§èƒ½å’Œç²¾åº¦ï¼‰
- **max-model-len**: 8192 ï¼ˆæœ€å¤§åºåˆ—é•¿åº¦ï¼‰
- **gpu-memory-utilization**: 0.95 ï¼ˆä½¿ç”¨95%æ˜¾å­˜ï¼‰

### ç¯å¢ƒå˜é‡

æ‰€æœ‰ç¯å¢ƒå˜é‡é…ç½®åœ¨ `vllm_env.sh` ä¸­ï¼š

```bash
source vllm_env.sh  # åŠ è½½ç¯å¢ƒå˜é‡
```

## ç›®å½•ç»“æ„å»ºè®®

```
/data/models/deepseek-v3/     # æ¨¡å‹å­˜å‚¨
/home/user/work/H20_install/  # å·¥ä½œç›®å½•
â”œâ”€â”€ vllm-env/                 # Pythonè™šæ‹Ÿç¯å¢ƒ
â”œâ”€â”€ vllm_env.sh              # ç¯å¢ƒå˜é‡é…ç½®
â”œâ”€â”€ vllm_server.log          # æœåŠ¡æ—¥å¿—
â””â”€â”€ benchmark_results_*.json # æ€§èƒ½æµ‹è¯•ç»“æœ
```

## æ•…éšœæ’æŸ¥

### é—®é¢˜1: GPUæ˜¾å­˜ä¸è¶³

```bash
# é™ä½æ˜¾å­˜ä½¿ç”¨ç‡
--gpu-memory-utilization 0.85

# å‡å°‘åºåˆ—é•¿åº¦
--max-model-len 4096
```

### é—®é¢˜2: ç«¯å£è¢«å ç”¨

```bash
# æŸ¥çœ‹å ç”¨è¿›ç¨‹
lsof -i :8000

# ä½¿ç”¨å…¶ä»–ç«¯å£
./start_inference.sh  # ç„¶åè¾“å…¥æ–°ç«¯å£å·
```

### é—®é¢˜3: æ¨¡å‹ä¸‹è½½å¤±è´¥

```bash
# ä½¿ç”¨é•œåƒç«™ç‚¹
export HF_ENDPOINT=https://hf-mirror.com
./download_model.sh
```

### é—®é¢˜4: å¤šGPUé€šä¿¡é”™è¯¯

```bash
# æ£€æŸ¥GPUæ‹“æ‰‘
nvidia-smi topo -m

# æŸ¥çœ‹NCCLæ—¥å¿—
export NCCL_DEBUG=INFO
```

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **å¯ç”¨FlashAttention**: å·²åœ¨ `vllm_env.sh` ä¸­é…ç½®
2. **è°ƒæ•´batch size**: ä½¿ç”¨ `benchmark.py` æ‰¾åˆ°æœ€ä½³æ‰¹æ¬¡å¤§å°
3. **ä¼˜åŒ–NCCL**: é…ç½® `NCCL_*` ç¯å¢ƒå˜é‡
4. **ä½¿ç”¨SSDå­˜å‚¨**: åŠ å¿«æ¨¡å‹åŠ è½½é€Ÿåº¦

## ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

å¯¹äºç”Ÿäº§ç¯å¢ƒï¼Œå»ºè®®ä½¿ç”¨systemdæœåŠ¡ç®¡ç†ï¼š

```bash
# å‚è€ƒæ–‡æ¡£ä¸­çš„systemdé…ç½®
sudo nano /etc/systemd/system/vllm-deepseek.service

# å¯åŠ¨æœåŠ¡
sudo systemctl start vllm-deepseek
sudo systemctl enable vllm-deepseek

# æŸ¥çœ‹çŠ¶æ€
sudo systemctl status vllm-deepseek

# æŸ¥çœ‹æ—¥å¿—
journalctl -u vllm-deepseek -f
```

## æŠ€æœ¯æ”¯æŒ

é‡åˆ°é—®é¢˜è¯·å‚è€ƒï¼š
1. `H20_VLLM_DeepSeek_V3_å®‰è£…æŒ‡å—.md` ä¸­çš„è¯¦ç»†æ–‡æ¡£
2. [vLLMå®˜æ–¹æ–‡æ¡£](https://docs.vllm.ai/)
3. [Deepseek V3æ¨¡å‹é¡µé¢](https://huggingface.co/deepseek-ai/DeepSeek-V3)

## ç³»ç»Ÿè¦æ±‚

- **GPU**: NVIDIA H20 x 8 (æ¯å¡141GBæ˜¾å­˜)
- **å†…å­˜**: 512GB+
- **å­˜å‚¨**: 500GB+ SSD
- **æ“ä½œç³»ç»Ÿ**: Ubuntu 22.04 LTS
- **Python**: 3.10-3.13 (vLLM 0.13.0è¦æ±‚)
- **CUDA**: 13.0
- **NVIDIAé©±åŠ¨**: 580.65.06+
- **PyTorch**: 2.9.0+cu130 (CUDA 13.0ç‰ˆæœ¬)
- **vLLM**: 0.13.0+

## è®¸å¯è¯

æœ¬å·¥å…·åŒ…éµå¾ªMITè®¸å¯è¯ã€‚ä½¿ç”¨çš„ç¬¬ä¸‰æ–¹è½¯ä»¶è¯·éµå¾ªå…¶å„è‡ªçš„è®¸å¯è¯ï¼š
- vLLM: Apache 2.0
- Deepseek V3: è¯·æŸ¥çœ‹æ¨¡å‹è®¸å¯è¯

---

ç¥éƒ¨ç½²é¡ºåˆ©ï¼ğŸš€
